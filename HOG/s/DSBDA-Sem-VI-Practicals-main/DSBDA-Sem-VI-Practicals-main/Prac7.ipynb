{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daf210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f32be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cff0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Natural Language Processing (NLP) helps computers understand human language. It is widely used in sentiment analysis, chatbots, and text summarization.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac63369",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9936a460",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2496d162",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f781d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"How to remove stop words with NLTK library in Python7\"\n",
    "text= re.sub('[^a-zA-Z]', ' ', text)\n",
    "tokens= word_tokenize(text.lower())\n",
    "filtered_text= []\n",
    "for w in tokens:\n",
    "  if w not in stop_words:\n",
    "    filtered_text.append(w)\n",
    "print(\"Tokenized Sentence:\", tokens)\n",
    "print(\"Filtered Sentence:\", filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb216bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "stemmed_words = ['wait', 'waiting', 'waited', 'waits']\n",
    "for w in stemmed_words:\n",
    "  print(w, \":\", ps.stem(w))\n",
    "print(\"Original Sentence:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb70e2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "text= \"studies studying cies cry\"\n",
    "tokenization= nltk.word_tokenize(text)\n",
    "for w in tokenization:\n",
    "  print(\"Lemma for {} is {}\".format(w, lemmatizer.lemmatize(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cb6ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'The pink sweater fits her perfectly'\n",
    "words = nltk.word_tokenize(data)\n",
    "for word in words:\n",
    "  print(nltk.pos_tag([word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b82e8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph= \"\"\"India is a vast country with second highest populati\n",
    " on in the world. It is a country\n",
    " with diverse cultures, traditions and beliefs. People in India cel\n",
    " ebrate unity in diversity.\n",
    " Festivals like Diwali, Holi, Navratri, Ramzan, Christmas etc. are \n",
    "celebrated by people across India \n",
    "and create a sense of brotherhood and cultural unity. Each festiva\n",
    " l has its religious and cultural importance.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f2370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = WordNetLemmatizer()\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "  review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "  review = review.lower()\n",
    "  review = review.split()\n",
    "  review = [wn.lemmatize(word) for word in review if not word in set(stopwords.words('their'))]\n",
    "  review = ' '.join(review)\n",
    "  corpus.append(review)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2307cbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(corpus).toarray()\n",
    "print(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
