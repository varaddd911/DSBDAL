{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb94b304b75ad3a2",
   "metadata": {},
   "source": [
    "# Madhur Jaripatke\n",
    "### Roll No. 48\n",
    "### TE A Computer\n",
    "### RMDSSOE, Warje, Pune\n",
    "\n",
    "###  7. Text Analytics\n",
    "1. Extract Sample document and apply following document preprocessing methods:\n",
    "Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
    "2. Create representation of document by calculating Term Frequency and Inverse Document \n",
    "Frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d1ea8537811eb",
   "metadata": {},
   "source": [
    "# Install & Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "324c3aa21460fdb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T23:43:05.576524Z",
     "start_time": "2024-04-07T23:43:03.872392Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\madhu\\pycharmprojects\\sppu-sem-vi-practicals\\dsbdal\\.venv\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\madhu\\pycharmprojects\\sppu-sem-vi-practicals\\dsbdal\\.venv\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\madhu\\pycharmprojects\\sppu-sem-vi-practicals\\dsbdal\\.venv\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\madhu\\pycharmprojects\\sppu-sem-vi-practicals\\dsbdal\\.venv\\lib\\site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\madhu\\pycharmprojects\\sppu-sem-vi-practicals\\dsbdal\\.venv\\lib\\site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\madhu\\pycharmprojects\\sppu-sem-vi-practicals\\dsbdal\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "199d529d86f7d213",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T23:43:05.581940Z",
     "start_time": "2024-04-07T23:43:05.576524Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9064d6328d71c0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T23:43:05.646997Z",
     "start_time": "2024-04-07T23:43:05.581940Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\madhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\madhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\madhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\madhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1c3b7a7b627257",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e3b90cad09f7b6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T23:43:05.652115Z",
     "start_time": "2024-04-07T23:43:05.646997Z"
    }
   },
   "outputs": [],
   "source": [
    "text=\"Tokenization replaces a sensitive data element, for example, a bank account number, with a non-sensitive substitute, known as a token. The token is a randomized data string that has no essential or exploitable value or meaning. It is a unique identifier which retains all the pertinent information about the data without compromising its security\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe2317ebcc4a6ad6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T23:43:12.896103Z",
     "start_time": "2024-04-07T23:43:12.886180Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization replaces a sensitive data element, for example, a bank account number, with a non-sensitive substitute, known as a token.', 'The token is a randomized data string that has no essential or exploitable value or meaning.', 'It is a unique identifier which retains all the pertinent information about the data without compromising its security']\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12823bcb790e3bf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T23:43:41.970049Z",
     "start_time": "2024-04-07T23:43:41.965915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'replaces', 'a', 'sensitive', 'data', 'element', ',', 'for', 'example', ',', 'a', 'bank', 'account', 'number', ',', 'with', 'a', 'non-sensitive', 'substitute', ',', 'known', 'as', 'a', 'token', '.', 'The', 'token', 'is', 'a', 'randomized', 'data', 'string', 'that', 'has', 'no', 'essential', 'or', 'exploitable', 'value', 'or', 'meaning', '.', 'It', 'is', 'a', 'unique', 'identifier', 'which', 'retains', 'all', 'the', 'pertinent', 'information', 'about', 'the', 'data', 'without', 'compromising', 'its', 'security']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a768bd5e92ba17",
   "metadata": {},
   "source": [
    "# POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4f8d23338c5d33e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T23:43:55.509198Z",
     "start_time": "2024-04-07T23:43:55.495678Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'should', \"didn't\", 'weren', 'be', 'hers', 'they', 'no', 'shan', 'won', 'yours', 'yourselves', 'after', 's', 'to', \"wasn't\", \"weren't\", 'its', 'themselves', 'same', 'o', 'being', 'how', 't', 'ain', 'aren', 'have', 'about', 'was', 'do', 'until', 'did', 'doesn', 'down', 'it', 'most', 'himself', 'very', 'the', 'of', 'our', 'i', 'such', \"hadn't\", \"you're\", 'if', 'ma', 'off', 'above', 'who', 'not', 'further', \"won't\", 'itself', 'few', 'is', \"you'd\", 'does', 'as', 'with', 'once', 'isn', 'herself', 'those', 'all', 'were', 'don', 'below', 'both', 'too', 'why', 'some', 'during', 'are', 'whom', \"aren't\", 'just', \"you've\", 'yourself', 'am', \"doesn't\", \"it's\", 'when', 'while', 'under', 'been', 'myself', 'then', \"don't\", \"wouldn't\", 'she', 'we', 'more', 'up', 'your', 'and', 'against', 'out', 'each', \"hasn't\", 'any', 'own', 'what', 'you', 'again', 'at', 'him', 'only', \"that'll\", 'from', 'couldn', 'than', 'having', 'ours', 'my', 'that', 'before', 'he', 'but', \"needn't\", 'or', 'didn', 'ourselves', \"you'll\", 'haven', 'between', 'wasn', \"she's\", \"mightn't\", 'their', 'so', 'can', 'a', 'through', 'now', 'here', \"isn't\", 'wouldn', 'me', 'into', 'where', 'for', 'will', 'm', 'needn', \"shouldn't\", 'y', 'an', 'mightn', 'other', 'had', \"should've\", 'doing', 'because', \"mustn't\", 'theirs', 're', 'in', 'by', 'on', 'hadn', \"shan't\", 'll', \"couldn't\", 'hasn', 'shouldn', 'has', 've', 'nor', 'her', 'these', 'them', 'which', 'mustn', 'd', \"haven't\", 'over', 'there', 'his', 'this'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260b1a410629f63a",
   "metadata": {},
   "source": [
    "# Stop Words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15c3fd73ac89c6e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T23:44:52.181348Z",
     "start_time": "2024-04-07T23:44:52.175659Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: ['how', 'to', 'remove', 'stop', 'words', 'with', 'nltk', 'library', 'in', 'python']\n",
      "Filtered Sentence: ['remove', 'stop', 'words', 'nltk', 'library', 'python']\n"
     ]
    }
   ],
   "source": [
    "text=\"How to remove stop words with NLTK library in Python7\"\n",
    "text= re.sub('[^a-zA-Z]', ' ', text)\n",
    "tokens= word_tokenize(text.lower())\n",
    "filtered_text= []\n",
    "for w in tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_text.append(w)\n",
    "print(\"Tokenized Sentence:\", tokens)\n",
    "print(\"Filtered Sentence:\", filtered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1404c82fc5bfd8a6",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc3afd3bdc3f67cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T23:45:44.955176Z",
     "start_time": "2024-04-07T23:45:44.945591Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait : wait\n",
      "waiting : wait\n",
      "waited : wait\n",
      "waits : wait\n",
      "Original Sentence: ['wait', 'waiting', 'waited', 'waits']\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "stemmed_words = ['wait', 'waiting', 'waited', 'waits']\n",
    "for w in stemmed_words:\n",
    "    print(w, \":\", ps.stem(w))\n",
    "print(\"Original Sentence:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dacf6565bcd368",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9e30e156d14092b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T23:47:15.672414Z",
     "start_time": "2024-04-07T23:47:14.750826Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for studies is study\n",
      "Lemma for studying is studying\n",
      "Lemma for cies is cies\n",
      "Lemma for cry is cry\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "text= \"studies studying cies cry\"\n",
    "tokenization= nltk.word_tokenize(text)\n",
    "for w in tokenization:\n",
    "    print(\"Lemma for {} is {}\".format(w, lemmatizer.lemmatize(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7564dda06abdd410",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T23:49:49.482852Z",
     "start_time": "2024-04-07T23:49:49.383923Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT')]\n",
      "[('pink', 'NN')]\n",
      "[('sweater', 'NN')]\n",
      "[('fits', 'NNS')]\n",
      "[('her', 'PRP$')]\n",
      "[('perfectly', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "data = 'The pink sweater fits her perfectly'\n",
    "words = nltk.word_tokenize(data)\n",
    "for word in words:\n",
    "    print(nltk.pos_tag([word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47645ad09fb1c671",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T23:50:18.988864Z",
     "start_time": "2024-04-07T23:50:18.983934Z"
    }
   },
   "outputs": [],
   "source": [
    "paragraph= \"\"\"India is a vast country with second highest populati\n",
    "on in the world. It is a country\n",
    "with diverse cultures, traditions and beliefs. People in India cel\n",
    "ebrate unity in diversity.\n",
    "Festivals like Diwali, Holi, Navratri, Ramzan, Christmas etc. are \n",
    "celebrated by people across India \n",
    "and create a sense of brotherhood and cultural unity. Each festiva\n",
    "l has its religious and cultural importance.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df0f0e6adb4b3c0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T23:53:18.559094Z",
     "start_time": "2024-04-07T23:53:18.535507Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['india vast country second highest populati world', 'country diverse culture tradition belief', 'people india cel ebrate unity diversity', 'festival like diwali holi navratri ramzan christmas etc', 'celebrated people across india create sense brotherhood cultural unity', 'festiva l religious cultural importance']\n"
     ]
    }
   ],
   "source": [
    "wn = WordNetLemmatizer()\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [wn.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf09f28c4f5c8b7",
   "metadata": {},
   "source": [
    "# Term Frequency and Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ac52d395e978ae3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T23:53:46.174116Z",
     "start_time": "2024-04-07T23:53:46.149720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.33061545 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.40318254\n",
      "  0.         0.         0.27912828 0.         0.         0.\n",
      "  0.40318254 0.         0.         0.40318254 0.         0.\n",
      "  0.         0.40318254 0.40318254]\n",
      " [0.         0.46262479 0.         0.         0.         0.\n",
      "  0.37935895 0.         0.         0.46262479 0.46262479 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.46262479\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.45529187 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.45529187\n",
      "  0.         0.45529187 0.         0.         0.         0.\n",
      "  0.         0.         0.31520422 0.         0.         0.37334585\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.37334585 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.35355339\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.35355339 0.         0.35355339 0.         0.35355339 0.\n",
      "  0.35355339 0.         0.         0.35355339 0.35355339 0.\n",
      "  0.         0.35355339 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.36523197 0.         0.36523197 0.         0.36523197 0.\n",
      "  0.         0.36523197 0.29949544 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.25285463 0.         0.         0.29949544\n",
      "  0.         0.         0.         0.         0.36523197 0.\n",
      "  0.29949544 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.42790272 0.         0.         0.\n",
      "  0.         0.         0.         0.52182349 0.         0.\n",
      "  0.         0.52182349 0.         0.         0.         0.\n",
      "  0.         0.         0.52182349 0.         0.         0.\n",
      "  0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(corpus).toarray()\n",
    "print(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
